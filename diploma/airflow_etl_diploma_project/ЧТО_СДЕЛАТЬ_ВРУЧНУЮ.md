# Что нужно сделать вручную

Всё, что можно, уже подготовлено в проекте. Ниже — только ваши действия.

---

## 1. Создать файл `.env`

**Один из способов:**

- **PowerShell** (в папке проекта):
  ```powershell
  Copy-Item env.ready .env
  ```
  или запустить скрипт:
  ```powershell
  .\copy_env.ps1
  ```

- **CMD** (в папке проекта):
  ```cmd
  copy env.ready .env
  ```
  или:
  ```cmd
  copy_env.bat
  ```

- **Вручную:** скопировать содержимое файла `env.ready` в новый файл с именем `.env` и сохранить в корне проекта.

После этого **ничего в `.env` менять не нужно** для локального Docker — пароли уже подставлены (source_pass, analytics_pass, admin и т.д.).

---

## 2. Запустить контейнеры

В терминале, в папке проекта:

```powershell
cd c:\Users\Taisiya\etl\diploma\airflow_etl_diploma_project
docker compose up -d
```

Подождать 1–2 минуты. Проверить:

```powershell
docker compose ps
```

Все сервисы должны быть в состоянии **Up**.

---

## 3. Открыть Airflow и проверить Connections

1. В браузере открыть: **http://localhost:8080**
2. Логин: **admin**, пароль: **admin** (из `.env`).
3. Зайти: **Admin** → **Connections**.
4. Убедиться, что есть: `postgres_source`, `postgres_analytics`, `mongo_source`, `ftp_server`.

Если Connections нет — один раз выполнить в терминале:

```powershell
docker compose exec airflow python /opt/airflow/scripts/init_connections.py
```

Обновить страницу Connections в браузере.

---

## 4. Сгенерировать тестовые данные

1. В Airflow на странице **DAGs** найти **generate_test_data**.
2. Включить DAG (переключатель **ON**).
3. Нажать **Trigger DAG** (справа).
4. Дождаться, пока все задачи станут зелёными (1–3 минуты).

---

## 5. Запустить основной ETL

1. Найти DAG **main_etl**.
2. Включить его (переключатель **ON**).
3. Нажать **Trigger DAG**.
4. Дождаться успешного завершения всех задач.

---

## 6. (По желанию) Открыть дашборд в Grafana

1. В браузере открыть: **http://localhost:3000**
2. Логин: **admin**, пароль: **admin**.
3. **Connections** → **Data sources** → **Add data source** → **PostgreSQL**:
   - Host: `postgres-analytics:5432`
   - Database: `analytics_db`
   - User: `analytics_user`
   - Password: `analytics_pass`
   - **Save & test**.
4. **Dashboards** → **Import** → загрузить файл `grafana/dashboards/business_analytics.json` из папки проекта → выбрать созданный источник → **Import**.

---

## Кратко

| Шаг | Действие |
|-----|----------|
| 1 | Скопировать `env.ready` в `.env` (команда или вручную). |
| 2 | Выполнить `docker compose up -d`, подождать 1–2 мин. |
| 3 | Открыть http://localhost:8080, проверить Connections (при необходимости запустить `init_connections.py`). |
| 4 | Включить и запустить DAG **generate_test_data**, дождаться зелёного. |
| 5 | Включить и запустить DAG **main_etl**, дождаться зелёного. |
| 6 | (Опционально) Настроить Grafana и импортировать дашборд. |

Добавлять что-то в `.env` не требуется — в `env.ready` уже прописаны рабочие значения для локального запуска.
