# Подробная инструкция: что сделать дальше

Пошаговый план от нуля до работающего ETL и дашборда.

---

## Шаг 1. Создать файл `.env`

Без `.env` контейнеры не получат пароли и хосты. **Пароли в коде не хранятся** — только в переменных окружения.

### 1.1. Скопировать пример

В папке проекта `airflow_etl_diploma_project` выполните в терминале:

```bash
cp .env.example .env
```

(В Windows PowerShell: `Copy-Item .env.example .env`)

### 1.2. Открыть `.env` в редакторе

Файл `.env` должен лежать рядом с `docker-compose.yml`.

### 1.3. Что обязательно заполнить

Минимум для запуска — **поставить свои пароли** (или оставить из примера только для локального теста):

| Переменная | Описание | Пример значения |
|------------|----------|-----------------|
| `POSTGRES_SOURCE_PASSWORD` | Пароль БД-источника (заказы, клиенты) | `source_pass` или любой пароль |
| `POSTGRES_ANALYTICS_PASSWORD` | Пароль аналитической БД и DWH | `analytics_pass` или любой пароль |
| `AIRFLOW_POSTGRES_PASSWORD` | Пароль БД самого Airflow | `airflow` |
| `MONGO_ROOT_PASSWORD` | Пароль root MongoDB | `mongo` или любой |
| `MONGO_PASSWORD` | Пароль пользователя MongoDB для приложения | `user_pass` или любой |
| `FTP_PASSWORD` | Пароль FTP (если используете) | `user` или любой |

Остальные переменные (хосты, порты, имена БД) можно **оставить как в `.env.example`** — они уже подходят для Docker.

Важно:

- Файл `.env` **не коммитить в Git** (он уже в `.gitignore`).
- В `docker-compose` порты **5433, 5434** — это порты на **вашем компьютере**; внутри контейнеров PostgreSQL всегда слушает 5432. В `.env` вы задаёте хосты имён сервисов (`postgres-source`, `postgres-analytics`), а не localhost.

---

## Шаг 2. Запустить Docker

### 2.1. Открыть терминал в папке проекта

Перейдите в каталог, где лежат `docker-compose.yml` и `.env`:

```bash
cd c:\Users\Taisiya\etl\diploma\airflow_etl_diploma_project
```

### 2.2. Запустить все сервисы

```bash
docker compose up -d
```

Будут подняты:

- PostgreSQL для Airflow (внутренняя БД)
- PostgreSQL Source (заказы, клиенты)
- PostgreSQL Analytics (аналитика + DWH)
- MongoDB (отзывы)
- FTP-сервер
- Airflow (webserver + scheduler)
- Grafana

### 2.3. Подождать 1–2 минуты

Пока инициализируются БД и Airflow. Проверить статус:

```bash
docker compose ps
```

Все сервисы должны быть в состоянии `Up` (или `running`). Если Airflow перезапускается — подождать ещё минуту.

### 2.4. Проверить логи Airflow (если что-то не поднимается)

```bash
docker compose logs -f airflow
```

В логах не должно быть постоянных ошибок подключения к БД. В конце должно быть что-то вроде «Booting worker» и «Listening at…». Выйти из логов: `Ctrl+C`.

---

## Шаг 3. Убедиться, что Connections созданы в Airflow

При старте контейнер Airflow запускает скрипт `scripts/init_connections.py` — он создаёт подключения из переменных из `.env`.

### 3.1. Открыть Airflow UI

В браузере: **http://localhost:8080**

Логин и пароль — из `.env`:

- `AIRFLOW_WWW_USER_USERNAME` (по умолчанию `admin`)
- `AIRFLOW_WWW_USER_PASSWORD` (по умолчанию `admin`)

### 3.2. Проверить Connections

1. В меню сверху: **Admin** → **Connections**.
2. Должны быть подключения:
   - `postgres_source`
   - `postgres_analytics`
   - `mongo_source`
   - `ftp_server`
   - (опционально) `api_service`

Если каких-то нет — создать их вручную (см. ниже) или запустить скрипт вручную.

### 3.3. Создать Connections вручную (если скрипт не сработал)

В **Admin → Connections → +** добавьте:

**postgres_source**

- Connection Id: `postgres_source`
- Connection Type: `Postgres`
- Host: `postgres-source`
- Schema: `source_db`
- Login: `source_user`
- Password: значение `POSTGRES_SOURCE_PASSWORD` из `.env`
- Port: `5432`
- Extra: `{"sslmode": "disable"}` (опционально)

**postgres_analytics**

- Connection Id: `postgres_analytics`
- Connection Type: `Postgres`
- Host: `postgres-analytics`
- Schema: `analytics_db`
- Login: `analytics_user`
- Password: значение `POSTGRES_ANALYTICS_PASSWORD` из `.env`
- Port: `5432`

**mongo_source**

- Connection Id: `mongo_source`
- Connection Type: `MongoDB`
- Host: `mongo`
- Schema: `feedback_db`
- Login: из `.env` (`MONGO_USER`, часто `user` или `admin`)
- Password: из `.env` (`MONGO_PASSWORD`)
- Port: `27017`
- Extra: `{"authSource": "admin"}`

### 3.4. Запустить скрипт создания Connections вручную (альтернатива)

Если вы уже подняли контейнеры и заполнили `.env`, можно один раз выполнить:

```bash
docker compose exec airflow python /opt/airflow/scripts/init_connections.py
```

Скрипт читает переменные окружения **из контейнера** (они передаются из `.env` через `docker-compose`). После этого снова проверить **Admin → Connections**.

---

## Шаг 4. Сгенерировать тестовые данные (обязательно перед первым ETL)

В пустых БД ETL нечего забирать. DAG `generate_test_data` создаёт клиентов, заказы, отзывы и CSV с продуктами.

### 4.1. Открыть список DAG

В Airflow UI: **DAGs** (главная страница).

### 4.2. Найти DAG `generate_test_data`

Включите его переключателем (ON), если выключен.

### 4.3. Запустить вручную

Нажмите на название DAG → кнопка **Trigger DAG** (справа).

### 4.4. Дождаться успешного завершения

Задачи по очереди: клиенты → заказы → отзывы → CSV продуктов → логи доставки → сводка. Все кружки должны стать зелёными. Обычно 1–3 минуты.

После этого в БД есть данные за «вчера» и «сегодня», и основной ETL сможет их подхватить.

---

## Шаг 5. Запустить основной ETL DAG (`main_etl`)

### 5.1. Включить DAG `main_etl`

На странице DAGs найдите **main_etl** и включите его (переключатель ON).

### 5.2. Запустить вручную (первый раз)

Нажмите **Trigger DAG**. Так вы не ждёте 9:00 утра.

### 5.3. Что делает DAG

1. **Extract** (параллельно):
   - из PostgreSQL: заказы за вчера, клиенты и позиции заказов по этим заказам;
   - из MongoDB: отзывы за вчера;
   - из CSV: продукты (последние файлы за 7 дней);
   - из FTP: логи доставки (если есть файлы);
   - из API (опционально).
2. **Validate** — проверка объёмов данных.
3. **Transform** — валидация заказов, нормализация, дедупликация.
4. **Load** (параллельно):
   - в аналитическую БД: агрегаты за день в `daily_business_analytics`;
   - в DWH: обновление `dim_customers` и `dim_products` (SCD Type 2), загрузка `fact_orders` с привязкой `customer_key` на дату заказа.

### 5.4. Проверить результат

- Все задачи зелёные — ETL прошёл успешно.
- В логах задач не должно быть traceback. При ошибке откройте задачу → **Log** и посмотрите текст ошибки (подключение, отсутствующая таблица, неверный тип данных и т.д.).

Дальше DAG будет автоматически запускаться **каждый день в 9:00** по расписанию.

---

## Шаг 6. Настроить дашборд в Grafana

### 6.1. Открыть Grafana

В браузере: **http://localhost:3000**

Логин/пароль — из `.env`: по умолчанию `admin` / `admin` (переменные `GF_SECURITY_ADMIN_USER`, `GF_SECURITY_ADMIN_PASSWORD`).

### 6.2. Добавить источник данных PostgreSQL

1. Меню слева: **Connections** (или **Configuration**) → **Data sources** → **Add data source**.
2. Выберите **PostgreSQL**.
3. Заполните:
   - **Host**: для Docker — `postgres-analytics:5432` (имя сервиса из docker-compose). Если Grafana запускается не в Docker — укажите `host.docker.internal:5434` или `localhost:5434` в зависимости от ОС.
   - **Database**: `analytics_db`
   - **User**: `analytics_user`
   - **Password**: тот же, что в `.env` для `POSTGRES_ANALYTICS_PASSWORD`
   - **TLS/SSL**: отключить (если не настраивали SSL).
4. Внизу нажмите **Save & test**. Должно быть сообщение «Database connection successful».

### 6.3. Импортировать готовый дашборд

1. Меню слева: **Dashboards** → **Import**.
2. Нажмите **Upload JSON file** и выберите файл:
   `grafana/dashboards/business_analytics.json`
   из папки проекта.
3. В поле **Data source** выберите только что созданный PostgreSQL.
4. Нажмите **Import**.

Должен открыться дашборд с панелями: Total Revenue (30 days), Total Orders (30 days), Avg Order Value, тренд по дням. Данные появятся после того, как хотя бы один раз успешно отработал ETL и заполнил `daily_business_analytics`.

### 6.4. Добавить свои панели (по желанию)

В папке `grafana/README.md` описаны примеры SQL для панелей (выручка, заказы по статусам, топ городов из DWH, рейтинг и т.д.). Можно создавать новые панели и вставлять эти запросы.

---

## Краткий чек-лист

- [ ] Создан файл `.env` из `.env.example`, заполнены пароли (и при необходимости хосты).
- [ ] Выполнено `docker compose up -d`, все контейнеры в статусе Up.
- [ ] Открыт Airflow http://localhost:8080, проверены Connections (postgres_source, postgres_analytics, mongo_source, ftp_server).
- [ ] Один раз запущен DAG `generate_test_data` и он завершился успешно.
- [ ] Включён и один раз запущен DAG `main_etl`, все задачи зелёные.
- [ ] В Grafana добавлен источник PostgreSQL (analytics_db), импортирован дашборд из `grafana/dashboards/business_analytics.json`.

---

## Частые проблемы

**Airflow не видит DAG `main_etl` или `generate_test_data`**  
- Проверьте, что папка `dags` примонтирована в контейнер (в `docker-compose` volumes: `./dags:/opt/airflow/dags`).  
- Посмотрите логи scheduler: `docker compose logs airflow` — не должно быть синтаксических ошибок в Python-файлах DAG.

**Ошибка подключения к PostgreSQL или MongoDB в задачах**  
- Проверьте, что в **Admin → Connections** указаны правильные Host (имена сервисов: `postgres-source`, `postgres-analytics`, `mongo`) и пароли из `.env`.  
- Убедитесь, что контейнеры БД уже подняты: `docker compose ps`.

**DAG падает на задаче `load_to_dwh`**  
- Убедитесь, что в аналитической БД выполнены скрипты инициализации (в т.ч. `init_dwh.sql` с таблицами `dim_*`, `fact_*` и заполнением `dim_date`, `dim_time`). При первом запуске они применяются автоматически при создании контейнера. Если БД создавали раньше без этих скриптов — нужно применить их вручную или пересоздать volume и контейнер.

**В Grafana «Database connection failed»**  
- Если Grafana в Docker, Host должен быть `postgres-analytics:5432`, а не `localhost:5434`.  
- Порт 5434 — это порт на вашей машине; внутри Docker-сети сервис слушает 5432.

**FTP или API не используются**  
- Это нормально. Задачи `extract_from_ftp` и `extract_from_api` при отсутствии данных или при ошибке возвращают пустой результат и не падают. ETL продолжает работать по остальным источникам (PostgreSQL, MongoDB, CSV).

Если опишете конкретную ошибку (из какой задачи, текст из Log в Airflow или сообщение Grafana), можно разобрать точечно.
